{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "sZb8Haf8XmAl"
   },
   "outputs": [],
   "source": [
    "### zAD 3\n",
    "\n",
    "##IMPL OF NN WITH K HIDDEN NODES (ONE LAYER), CHOICE OF NO, L1, L2, AND CHOICE RELU VS SIGMOID\n",
    "\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Split a dataset into k and mix them up\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\n",
    " dataset_split = list()\n",
    " dataset_copy = list(dataset)\n",
    "\n",
    " fold_size = int(len(dataset) / n_folds)\n",
    "\n",
    " for i in range(n_folds):\n",
    "  fold = list()\n",
    "\n",
    "  while len(fold) < fold_size:\n",
    "    index = randrange(len(dataset_copy))\n",
    "    fold.append(dataset_copy.pop(index))\n",
    "\n",
    "  dataset_split.append(fold)\n",
    "\n",
    " return dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    " correct = 0\n",
    "\n",
    " for i in range(len(actual)):\n",
    "  if actual[i] == predicted[i]:\n",
    "    correct += 1\n",
    "\n",
    " return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, normalization_method, *args):\n",
    "\n",
    " folds = cross_validation_split(dataset, n_folds)\n",
    " scores = list()\n",
    "\n",
    " for fold in folds:\n",
    "  train_set = list(folds)\n",
    "  train_set.remove(fold)\n",
    "  train_set = sum(train_set, [])\n",
    "  test_set = list()\n",
    "\n",
    "  for row in fold:\n",
    "    row_copy = list(row)\n",
    "    test_set.append(row_copy)\n",
    "    row_copy[-1] = None\n",
    "\n",
    "  predicted = algorithm(train_set, test_set, normalization_method, *args)\n",
    "  actual = [row[-1] for row in fold]\n",
    "  accuracy = accuracy_metric(actual, predicted)\n",
    "  scores.append(accuracy)\n",
    "\n",
    " return scores\n",
    " \n",
    "# Calc all neuron activation values\n",
    "def activate(weights, inputs):\n",
    " activation = weights[-1]\n",
    "\n",
    "\n",
    " for i in range(len(weights)-1):\n",
    "  activation += weights[i] * inputs[i]\n",
    " return activation\n",
    " \n",
    "# SIGMOID ACTIVATION FUNCTION\n",
    "def sigmoid(activation):\n",
    " try:\n",
    "    result = 1.0 / (1.0 + exp(-activation))\n",
    " except OverflowError:\n",
    "    result = float('inf')\n",
    " return result\n",
    "\n",
    "# RELU ACTIVATION FUNCTION\n",
    "def ReLU(activation):\n",
    "  return max(0, activation)\n",
    " \n",
    "def forward_propagate(network, row):\n",
    " inputs = row\n",
    " for layer in network:\n",
    "  new_inputs = []\n",
    "  for neuron in layer:\n",
    "    activation = activate(neuron['weights'], inputs)\n",
    "    \n",
    "    #USE SIGMOID\n",
    "    #neuron['output'] = sigmoid(activation)\n",
    "    #USE RELU \n",
    "    neuron['output'] = sigmoid(activation)\n",
    "\n",
    "    new_inputs.append(neuron['output'])\n",
    "  inputs = new_inputs\n",
    " return inputs\n",
    " \n",
    "def transfer_derivative(output):\n",
    " return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error \n",
    "def backward_propagate_error(network, expected):\n",
    " for i in reversed(range(len(network))):\n",
    "  layer = network[i]\n",
    "  errors = list()\n",
    "  if i != len(network)-1:\n",
    "    for j in range(len(layer)):\n",
    "      error = 0.0\n",
    "      for neuron in network[i + 1]:\n",
    "        error += (neuron['weights'][j] * neuron['delta'])\n",
    "      errors.append(error)\n",
    "  else:\n",
    "    for j in range(len(layer)):\n",
    "      neuron = layer[j]\n",
    "\n",
    "      error = neuron['output'] - expected[j]\n",
    "\n",
    "      norm_lambda1 = 0.001\n",
    "      norm_lambda2 = 0.0001\n",
    "\n",
    "      l1_norm = sum(sum(sum(abs(weight) for weight in neuron['weights']) for neuron in layer) for layer in network)\n",
    "      l2_norm = sum(sum(sum(weight**2 for weight in neuron['weights']) for neuron in layer) for layer in network)\n",
    "\n",
    "      if normalization_method == 0:\n",
    "        continue\n",
    "      elif normalization_method == 1:\n",
    "        error += norm_lambda1 * l1_norm\n",
    "      elif normalization_method == 2:\n",
    "        error += norm_lambda2 * l2_norm\n",
    "\n",
    "      errors.append(error)\n",
    "\n",
    "  for j in range(len(layer)):\n",
    "    neuron = layer[j]\n",
    "    neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    " for i in range(len(network)):\n",
    "  inputs = row[:-1]\n",
    "  if i != 0:\n",
    "    inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "  for neuron in network[i]:\n",
    "    for j in range(len(inputs)):\n",
    "      neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "    neuron['weights'][-1] -= l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs, normalization_method):\n",
    " for epoch in range(n_epoch):\n",
    "  sum_error = 0\n",
    "  for row in train:\n",
    "    outputs = forward_propagate(network, row)\n",
    "    expected = [0 for i in range(n_outputs)]\n",
    "    expected[row[-1]] = 1\n",
    "    \n",
    "    #mean squared error\n",
    "    sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\n",
    "    backward_propagate_error(network, expected)\n",
    "    update_weights(network, row, l_rate)\n",
    "  print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    " network = list()\n",
    " hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)], 'delta': 1} for i in range(n_hidden)]\n",
    " network.append(hidden_layer)\n",
    " output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)], 'delta': 1} for i in range(n_outputs)]\n",
    " network.append(output_layer)\n",
    " return network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    " outputs = forward_propagate(network, row)\n",
    " return outputs.index(max(outputs))\n",
    " \n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test,normalization_method,  l_rate, n_epoch, n_hidden):\n",
    " n_inputs = len(train[0]) - 1\n",
    " n_outputs = len(set([row[-1] for row in train]))   #[0, 1]\n",
    " print(n_outputs)\n",
    " network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    " train_network(network, train, l_rate, n_epoch, n_outputs, normalization_method)\n",
    " predictions = list()\n",
    " for row in test:\n",
    "  prediction = predict(network, row)\n",
    "  predictions.append(prediction)\n",
    " return(predictions)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCfT7qK2IVto",
    "outputId": "4fc44b49-e68b-4862-9815-d9f23322e367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ">epoch=0, lrate=0.300, error=413.435\n",
      ">epoch=1, lrate=0.300, error=405.000\n",
      ">epoch=2, lrate=0.300, error=381.576\n",
      ">epoch=3, lrate=0.300, error=297.507\n",
      ">epoch=4, lrate=0.300, error=224.132\n",
      ">epoch=5, lrate=0.300, error=196.741\n",
      ">epoch=6, lrate=0.300, error=186.969\n",
      ">epoch=7, lrate=0.300, error=182.906\n",
      ">epoch=8, lrate=0.300, error=180.995\n",
      ">epoch=9, lrate=0.300, error=180.017\n",
      "2\n",
      ">epoch=0, lrate=0.300, error=415.081\n",
      ">epoch=1, lrate=0.300, error=406.933\n",
      ">epoch=2, lrate=0.300, error=394.735\n",
      ">epoch=3, lrate=0.300, error=354.916\n",
      ">epoch=4, lrate=0.300, error=275.190\n",
      ">epoch=5, lrate=0.300, error=216.822\n",
      ">epoch=6, lrate=0.300, error=196.861\n",
      ">epoch=7, lrate=0.300, error=189.722\n",
      ">epoch=8, lrate=0.300, error=186.685\n",
      ">epoch=9, lrate=0.300, error=185.172\n",
      "2\n",
      ">epoch=0, lrate=0.300, error=417.528\n",
      ">epoch=1, lrate=0.300, error=407.036\n",
      ">epoch=2, lrate=0.300, error=405.279\n",
      ">epoch=3, lrate=0.300, error=399.882\n",
      ">epoch=4, lrate=0.300, error=377.669\n",
      ">epoch=5, lrate=0.300, error=321.563\n",
      ">epoch=6, lrate=0.300, error=250.076\n",
      ">epoch=7, lrate=0.300, error=211.790\n",
      ">epoch=8, lrate=0.300, error=197.135\n",
      ">epoch=9, lrate=0.300, error=190.962\n",
      "2\n",
      ">epoch=0, lrate=0.300, error=416.771\n",
      ">epoch=1, lrate=0.300, error=407.163\n",
      ">epoch=2, lrate=0.300, error=396.913\n",
      ">epoch=3, lrate=0.300, error=362.493\n",
      ">epoch=4, lrate=0.300, error=283.768\n",
      ">epoch=5, lrate=0.300, error=225.201\n",
      ">epoch=6, lrate=0.300, error=204.523\n",
      ">epoch=7, lrate=0.300, error=197.091\n",
      ">epoch=8, lrate=0.300, error=193.982\n",
      ">epoch=9, lrate=0.300, error=192.527\n",
      "2\n",
      ">epoch=0, lrate=0.300, error=417.241\n",
      ">epoch=1, lrate=0.300, error=409.964\n",
      ">epoch=2, lrate=0.300, error=407.695\n",
      ">epoch=3, lrate=0.300, error=397.053\n",
      ">epoch=4, lrate=0.300, error=354.755\n",
      ">epoch=5, lrate=0.300, error=272.365\n",
      ">epoch=6, lrate=0.300, error=220.754\n",
      ">epoch=7, lrate=0.300, error=202.696\n",
      ">epoch=8, lrate=0.300, error=195.987\n",
      ">epoch=9, lrate=0.300, error=193.090\n",
      "Scores: [84.0, 81.5, 82.5, 88.0, 84.5]\n",
      "Mean Accuracy: 84.100%\n"
     ]
    }
   ],
   "source": [
    "#EXPERIMENT PART\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_dataset(data_points):\n",
    "  dataset = []\n",
    "\n",
    "  for index in range(0, data_points):\n",
    "    v = []\n",
    "    first = random.uniform(-1, 1)\n",
    "    second = random.uniform(-1, 1)\n",
    "\n",
    "    if first == 0 or second == 0:\n",
    "      first = random.uniform(-1, 1)\n",
    "      second = random.uniform(-1, 1)\n",
    "\n",
    "    expected = 0 if (first * second) > 0 else 1\n",
    "\n",
    "    v.append(first)\n",
    "    v.append(second)\n",
    "    v.append(expected)\n",
    "    dataset.append(v)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "# SETTINGS\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 10\n",
    "n_hidden = 4\n",
    "\n",
    "normalization_method = 1\n",
    "normalization_lambda = 0.001\n",
    "\n",
    "# Prepare the data\n",
    "\n",
    "dataset = generate_dataset(1000)\n",
    "\n",
    "# evaluate algorithm\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, normalization_method, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
